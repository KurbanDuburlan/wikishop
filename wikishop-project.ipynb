{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af171589c0049cab9f820c165f78564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import spacy\n",
    "from spacy.lang.ru import Russian\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139299</th>\n",
       "      <td>139451</td>\n",
       "      <td>\"\\nThe \"\"relic\"\" is just an example. There are...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64866</th>\n",
       "      <td>64933</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128052</th>\n",
       "      <td>128184</td>\n",
       "      <td>Given Jmcnamera's history and some edit histor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57964</th>\n",
       "      <td>58028</td>\n",
       "      <td>OK, I think I'm done tweaking it.  I have an o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140315</th>\n",
       "      <td>140467</td>\n",
       "      <td>Ass you might want to Kiss\\n\\nMine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124475</th>\n",
       "      <td>124604</td>\n",
       "      <td>sorry Baseball Bugs. I'll try better next time...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47196</th>\n",
       "      <td>47251</td>\n",
       "      <td>No, no, no. He did not block me for that, that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133507</th>\n",
       "      <td>133645</td>\n",
       "      <td>theypere is no 14 Cylinder version yet.. i don...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57885</th>\n",
       "      <td>57949</td>\n",
       "      <td>Sore loser, Nawlinwiki!  I still got over 150 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139244</th>\n",
       "      <td>139396</td>\n",
       "      <td>I was attempting to make a Talk to Animals art...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  toxic\n",
       "139299      139451  \"\\nThe \"\"relic\"\" is just an example. There are...      0\n",
       "64866        64933  \"\\n\\n Please do not vandalize pages, as you di...      0\n",
       "128052      128184  Given Jmcnamera's history and some edit histor...      0\n",
       "57964        58028  OK, I think I'm done tweaking it.  I have an o...      0\n",
       "140315      140467                 Ass you might want to Kiss\\n\\nMine      1\n",
       "124475      124604  sorry Baseball Bugs. I'll try better next time...      0\n",
       "47196        47251  No, no, no. He did not block me for that, that...      0\n",
       "133507      133645  theypere is no 14 Cylinder version yet.. i don...      0\n",
       "57885        57949  Sore loser, Nawlinwiki!  I still got over 150 ...      1\n",
       "139244      139396  I was attempting to make a Talk to Animals art...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "\n",
    "Датасет содержит 159,292 записи и состоит из трех столбцов: \"Unnamed: 0\", \"text\" и \"toxic\". \n",
    "\n",
    "Столбец \"Unnamed: 0\" представляет собой индекс записей.\n",
    "\n",
    "Столбец \"text\" содержит текст комментариев.\n",
    "\n",
    "Столбец \"toxic\" указывает на то, является ли комментарий токсичным (1) или нетоксичным (0).\n",
    "\n",
    "Общая информация о датасете показывает, что все столбцы не содержат пропущенных значений.\n",
    "\n",
    "Пример первых 10 записей датасета показывает текст комментариев и соответствующую метку \"toxic\".\n",
    "\n",
    "На основе предоставленных данных можно провести дальнейший анализ и моделирование для классификации токсичных комментариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                               text  toxic\n",
      "0                0  Explanation Why edits make username Hardcore M...      0\n",
      "1                1  D'aww ! He match background colour I 'm seemin...      0\n",
      "2                2  Hey man , I 'm really try edit war . It 's guy...      0\n",
      "3                3  `` More I ca n't make real suggestion improvem...      0\n",
      "4                4   You , sir , hero . Any chance remember page 's ?      0\n",
      "...            ...                                                ...    ...\n",
      "159287      159446  `` : : : : : And second time ask , view comple...      0\n",
      "159288      159447  You ashamed That horrible thing put talk page ...      0\n",
      "159289      159448  Spitzer Umm , actual article prostitution ring...      0\n",
      "159290      159449  And look like actually put speedy first versio...      0\n",
      "159291      159450  `` And ... I really n't think understand . I c...      0\n",
      "\n",
      "[159292 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])\n",
    "    return lemmatized_output\n",
    "\n",
    "\n",
    "# Лемматизация столбца 'text' в датафрейме\n",
    "data['text'] = data['text'].apply(lemmatize_text)\n",
    "\n",
    "# Удаление стоп-слов и объединение токенов в строку\n",
    "stop_words = set(stopwords.words('english')) \n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([token for token in x.split() if token not in stop_words]))\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающий и тестовый наборы\n",
    "x = data['text']  # Очищенные тексты комментариев\n",
    "y = data['toxic']  # Целевой признак\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом коде мы используем модель LinearSVC из библиотеки scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация векторизатора\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Преобразование текста в векторы признаков\n",
    "train_features = vectorizer.fit_transform(x_train)\n",
    "test_features = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание модели LinearSVC\n",
    "model_svc = LinearSVC(random_state=12345)\n",
    "\n",
    "# Определение Pipeline\n",
    "pipeline_svc = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', LinearSVC(random_state=12345))\n",
    "])\n",
    "\n",
    "# Определение сетки параметров для GridSearchCV\n",
    "param_grid_svc = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'model__C': [0.01, 0.1, 1, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшая метрика F1-меры на кросс-валидации: 0.7886369704222373\n"
     ]
    }
   ],
   "source": [
    "# Создание объекта GridSearchCV для модели LinearSVC с Pipeline\n",
    "grid_search_svc = GridSearchCV(pipeline_svc, param_grid_svc, scoring='f1', cv=5)\n",
    "\n",
    "# Обучение модели LinearSVC с использованием GridSearchCV и Pipeline\n",
    "grid_search_svc.fit(x_train, y_train)\n",
    "\n",
    "# Получение значения наилучшей метрики F1-меры на кросс-валидации\n",
    "best_f1_score = grid_search_svc.best_score_\n",
    "print(\"Наилучшая метрика F1-меры на кросс-валидации:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, построим модель машинного обучения на основе Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание модели Logistic Regression\n",
    "model_lr = LogisticRegression(random_state=12345)\n",
    "\n",
    "# Определение Pipeline для модели Logistic Regression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression(random_state=12345))\n",
    "])\n",
    "\n",
    "# Определение сетки параметров для GridSearchCV\n",
    "param_grid_lr = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'model__C': [0.01, 0.1, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание объекта GridSearchCV для модели Logistic Regression с Pipeline\n",
    "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, scoring='f1', cv=5)\n",
    "\n",
    "# Обучение модели Logistic Regression с использованием GridSearchCV и Pipeline\n",
    "grid_search_lr.fit(x_train, y_train)\n",
    "\n",
    "# Получение значения наилучшей метрики F1-меры на кросс-валидации\n",
    "best_f1_score_lr = grid_search_lr.best_score_\n",
    "print(\"Наилучшая метрика F1-меры на кросс-валидации для модели Logistic Regression:\", best_f1_score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на тестовой выборке с использованием лучшей модели LinearSVC\n",
    "y_pred_svc_test = best_model_svc.predict(x_test)\n",
    "\n",
    "# Оценка производительности модели LinearSVC на тестовой выборке\n",
    "f1_svc_test = f1_score(y_test, y_pred_svc_test)\n",
    "print(\"F1-мера для модели LinearSVC на тестовой выборке:\", f1_svc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем применить обученную модель LinearSVC для классификации комментариев на позитивные и негативные. Затем мы можем отправить токсичные комментарии на модерацию для дальнейшей проверки и удаления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение модели LinearSVC на всем наборе данных\n",
    "all_comments_features = vectorizer.transform(data['text'])\n",
    "all_comments_pred = best_model_svc.predict(all_comments_features)\n",
    "\n",
    "# Создание нового столбца для предсказанных меток\n",
    "data['predicted_toxic'] = all_comments_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отправка токсичных комментариев на модерацию\n",
    "toxic_comments = data[data['predicted_toxic'] == 1]\n",
    "\n",
    "# Вывод примеров токсичных комментариев\n",
    "print(\"Примеры токсичных комментариев:\")\n",
    "for comment in toxic_comments['text'].head(5):\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте мы решали задачу модерации токсичных комментариев в интернет-магазине. Мы использовали набор данных, содержащий тексты комментариев и информацию о токсичности.\n",
    "\n",
    "В ходе проекта мы выполнили следующие шаги:\n",
    "\n",
    "Предварительная обработка данных: Мы провели очистку текста от ненужных символов и специальных символов, удаление стоп-слов и приведение текста к нормализованному виду.\n",
    "\n",
    "Разделение данных на обучающий и тестовый наборы: Мы разделили данные на обучающий и тестовый наборы для оценки производительности модели на неразмеченных данных.\n",
    "\n",
    "Создание модели: Мы построили модели глубокого обучения, такие как логистическая регрессия и модель LinearSVC. Модели были обучены на обучающем наборе данных.\n",
    "\n",
    "Оценка моделей: Мы оценили производительность моделей на тестовом наборе данных, вычислив метрику F1-меры. Для моделей логистической регрессии и LinearSVC использовали F1-меру.\n",
    "\n",
    "Модерация токсичных комментариев: Мы применили обученную модель LinearSVC для классификации комментариев на позитивные и негативные. Токсичные комментарии можно отправить на модерацию для дальнейшей проверки и удаления.\n",
    "\n",
    "В результате проведенного проекта были построены и оценены модели для модерации токсичных комментариев. Модель LinearSVC показала достаточно хорошую производительность с F1-мерой около 0.80, что свидетельствует о ее способности эффективно классифицировать комментарии на позитивные и негативные."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 10723,
    "start_time": "2023-07-10T01:21:59.332Z"
   },
   {
    "duration": 168,
    "start_time": "2023-07-10T01:23:25.381Z"
   },
   {
    "duration": 2540,
    "start_time": "2023-07-10T01:23:52.157Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-10T01:24:04.994Z"
   },
   {
    "duration": 32,
    "start_time": "2023-07-10T01:24:22.690Z"
   },
   {
    "duration": 265,
    "start_time": "2023-07-10T01:28:37.976Z"
   },
   {
    "duration": 1996,
    "start_time": "2023-07-10T01:28:57.137Z"
   },
   {
    "duration": 818,
    "start_time": "2023-07-10T01:28:59.135Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-10T01:28:59.955Z"
   },
   {
    "duration": 30,
    "start_time": "2023-07-10T01:28:59.965Z"
   },
   {
    "duration": 88388,
    "start_time": "2023-07-10T01:28:59.997Z"
   },
   {
    "duration": 1873,
    "start_time": "2023-07-10T01:30:33.179Z"
   },
   {
    "duration": 794,
    "start_time": "2023-07-10T01:30:35.054Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-10T01:30:35.850Z"
   },
   {
    "duration": 37,
    "start_time": "2023-07-10T01:30:35.861Z"
   },
   {
    "duration": 1817,
    "start_time": "2023-07-10T01:31:01.098Z"
   },
   {
    "duration": 899,
    "start_time": "2023-07-10T01:31:02.917Z"
   },
   {
    "duration": 17,
    "start_time": "2023-07-10T01:31:03.818Z"
   },
   {
    "duration": 30,
    "start_time": "2023-07-10T01:31:03.837Z"
   },
   {
    "duration": 22,
    "start_time": "2023-07-10T01:31:03.869Z"
   },
   {
    "duration": 101602,
    "start_time": "2023-07-10T01:31:03.893Z"
   },
   {
    "duration": 129,
    "start_time": "2023-07-10T01:37:31.321Z"
   },
   {
    "duration": 31,
    "start_time": "2023-07-10T01:37:40.795Z"
   },
   {
    "duration": 3181,
    "start_time": "2023-07-10T01:41:28.555Z"
   },
   {
    "duration": 813,
    "start_time": "2023-07-10T01:41:31.738Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-10T01:41:32.552Z"
   },
   {
    "duration": 43,
    "start_time": "2023-07-10T01:41:32.563Z"
   },
   {
    "duration": 22,
    "start_time": "2023-07-10T01:41:32.608Z"
   },
   {
    "duration": 102059,
    "start_time": "2023-07-10T01:41:32.632Z"
   },
   {
    "duration": 28,
    "start_time": "2023-07-10T01:43:14.693Z"
   },
   {
    "duration": 10017,
    "start_time": "2023-07-10T01:44:27.933Z"
   },
   {
    "duration": 1200,
    "start_time": "2023-07-10T01:44:41.395Z"
   },
   {
    "duration": 3222,
    "start_time": "2023-07-10T02:18:09.891Z"
   },
   {
    "duration": 800,
    "start_time": "2023-07-10T02:18:13.115Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-10T02:18:13.917Z"
   },
   {
    "duration": 36,
    "start_time": "2023-07-10T02:18:13.936Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-10T02:18:13.974Z"
   },
   {
    "duration": 101934,
    "start_time": "2023-07-10T02:18:13.983Z"
   },
   {
    "duration": 36,
    "start_time": "2023-07-10T02:19:55.918Z"
   },
   {
    "duration": 7016,
    "start_time": "2023-07-10T02:19:55.956Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-10T02:20:02.974Z"
   },
   {
    "duration": 32711,
    "start_time": "2023-07-10T02:20:02.981Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-10T02:20:35.694Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-10T02:20:35.712Z"
   },
   {
    "duration": 732301,
    "start_time": "2023-07-10T02:20:35.731Z"
   },
   {
    "duration": 103,
    "start_time": "2023-07-10T02:32:48.035Z"
   },
   {
    "duration": 6407,
    "start_time": "2023-07-10T02:42:44.880Z"
   },
   {
    "duration": 31,
    "start_time": "2023-07-10T02:42:54.500Z"
   },
   {
    "duration": 6627,
    "start_time": "2023-07-10T10:44:43.886Z"
   },
   {
    "duration": 4386,
    "start_time": "2023-07-10T10:44:50.515Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-10T10:44:54.903Z"
   },
   {
    "duration": 60,
    "start_time": "2023-07-10T10:44:54.921Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-10T10:44:54.984Z"
   },
   {
    "duration": 529,
    "start_time": "2023-07-10T10:44:54.999Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.531Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.533Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.535Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.536Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.554Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.555Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.582Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.589Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.590Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.591Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.591Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.592Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.594Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:44:55.595Z"
   },
   {
    "duration": 146,
    "start_time": "2023-07-10T10:47:30.083Z"
   },
   {
    "duration": 5919,
    "start_time": "2023-07-10T10:48:35.278Z"
   },
   {
    "duration": 1013,
    "start_time": "2023-07-10T10:48:41.200Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-10T10:48:42.215Z"
   },
   {
    "duration": 54,
    "start_time": "2023-07-10T10:48:42.227Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-10T10:48:42.293Z"
   },
   {
    "duration": 177,
    "start_time": "2023-07-10T10:48:42.306Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.486Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.488Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.490Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.491Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.493Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.494Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.496Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.497Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.499Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.500Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.502Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.503Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.504Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T10:48:42.506Z"
   },
   {
    "duration": 5295,
    "start_time": "2023-07-10T10:51:21.071Z"
   },
   {
    "duration": 964,
    "start_time": "2023-07-10T10:51:26.368Z"
   },
   {
    "duration": 40,
    "start_time": "2023-07-10T10:51:27.334Z"
   },
   {
    "duration": 39,
    "start_time": "2023-07-10T10:51:27.377Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-10T10:51:27.420Z"
   },
   {
    "duration": 12334,
    "start_time": "2023-07-10T10:51:27.433Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-10T10:51:39.770Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-10T10:51:39.778Z"
   },
   {
    "duration": 2943334,
    "start_time": "2023-07-10T10:51:39.786Z"
   },
   {
    "duration": 50,
    "start_time": "2023-07-10T11:40:43.122Z"
   },
   {
    "duration": 10248,
    "start_time": "2023-07-10T11:40:43.173Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-10T11:40:53.423Z"
   },
   {
    "duration": 3327912,
    "start_time": "2023-07-10T11:40:53.429Z"
   },
   {
    "duration": 12511,
    "start_time": "2023-07-10T12:36:21.345Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-10T12:36:33.874Z"
   },
   {
    "duration": 68,
    "start_time": "2023-07-10T12:48:19.684Z"
   },
   {
    "duration": 29299,
    "start_time": "2023-07-10T12:50:33.384Z"
   },
   {
    "duration": 7390,
    "start_time": "2023-07-10T12:51:02.685Z"
   },
   {
    "duration": 25,
    "start_time": "2023-07-10T12:51:10.081Z"
   },
   {
    "duration": 68,
    "start_time": "2023-07-10T12:51:10.120Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-10T12:51:10.199Z"
   },
   {
    "duration": 20581,
    "start_time": "2023-07-10T12:51:10.228Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-10T12:51:30.826Z"
   },
   {
    "duration": 92,
    "start_time": "2023-07-10T12:51:30.852Z"
   },
   {
    "duration": 3519244,
    "start_time": "2023-07-10T12:51:30.952Z"
   },
   {
    "duration": 31,
    "start_time": "2023-07-10T13:50:10.201Z"
   },
   {
    "duration": 11750,
    "start_time": "2023-07-10T13:50:10.234Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-10T13:50:21.990Z"
   },
   {
    "duration": 2955319,
    "start_time": "2023-07-10T13:50:21.996Z"
   },
   {
    "duration": 9753,
    "start_time": "2023-07-10T14:39:37.319Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-10T14:39:47.079Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T04:49:48.307Z"
   },
   {
    "duration": 13214,
    "start_time": "2023-07-11T04:50:01.911Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-11T04:51:15.136Z"
   },
   {
    "duration": 12052,
    "start_time": "2023-07-11T04:51:35.972Z"
   },
   {
    "duration": 16569,
    "start_time": "2023-07-11T06:50:04.779Z"
   },
   {
    "duration": 5067,
    "start_time": "2023-07-11T07:29:34.720Z"
   },
   {
    "duration": 2278,
    "start_time": "2023-07-11T07:29:39.789Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-11T07:29:42.068Z"
   },
   {
    "duration": 36,
    "start_time": "2023-07-11T07:29:42.081Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-11T07:29:42.119Z"
   },
   {
    "duration": 563560,
    "start_time": "2023-07-11T07:29:42.133Z"
   },
   {
    "duration": 321,
    "start_time": "2023-07-11T07:39:05.694Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.016Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.018Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.019Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.020Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.021Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.022Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.023Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.025Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:39:06.026Z"
   },
   {
    "duration": 6089,
    "start_time": "2023-07-11T07:49:05.154Z"
   },
   {
    "duration": 768,
    "start_time": "2023-07-11T07:49:11.246Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-11T07:49:12.016Z"
   },
   {
    "duration": 44,
    "start_time": "2023-07-11T07:49:12.030Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-11T07:49:12.076Z"
   },
   {
    "duration": 1729,
    "start_time": "2023-07-11T07:49:12.090Z"
   },
   {
    "duration": 4754,
    "start_time": "2023-07-11T07:49:13.820Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.576Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.578Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.579Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.580Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.581Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.582Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.583Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-11T07:49:18.584Z"
   },
   {
    "duration": 4977,
    "start_time": "2023-07-11T08:10:17.110Z"
   },
   {
    "duration": 777,
    "start_time": "2023-07-11T08:10:22.088Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-11T08:10:22.867Z"
   },
   {
    "duration": 40,
    "start_time": "2023-07-11T08:10:22.881Z"
   },
   {
    "duration": 29,
    "start_time": "2023-07-11T08:10:22.924Z"
   },
   {
    "duration": 1330222,
    "start_time": "2023-07-11T08:10:22.955Z"
   },
   {
    "duration": 33,
    "start_time": "2023-07-11T08:32:33.180Z"
   },
   {
    "duration": 6498,
    "start_time": "2023-07-11T08:32:33.215Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-11T08:32:39.715Z"
   },
   {
    "duration": 2143954,
    "start_time": "2023-07-11T08:32:39.722Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-11T09:08:23.682Z"
   },
   {
    "duration": 50,
    "start_time": "2023-07-11T09:31:04.798Z"
   },
   {
    "duration": 5448,
    "start_time": "2023-07-11T09:34:18.051Z"
   },
   {
    "duration": 2447,
    "start_time": "2023-07-11T09:34:23.501Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-11T09:34:25.950Z"
   },
   {
    "duration": 50,
    "start_time": "2023-07-11T09:34:25.966Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-11T09:34:26.019Z"
   },
   {
    "duration": 1357662,
    "start_time": "2023-07-11T09:34:26.037Z"
   },
   {
    "duration": 32,
    "start_time": "2023-07-11T09:57:03.701Z"
   },
   {
    "duration": 6451,
    "start_time": "2023-07-11T09:57:03.734Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-11T09:57:10.187Z"
   },
   {
    "duration": 2264202,
    "start_time": "2023-07-11T09:57:10.195Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-11T10:34:54.399Z"
   },
   {
    "duration": 17562,
    "start_time": "2023-07-11T10:42:40.461Z"
   },
   {
    "duration": 3475,
    "start_time": "2023-07-11T10:43:05.042Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-11T10:43:08.520Z"
   },
   {
    "duration": 44,
    "start_time": "2023-07-11T10:43:11.474Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-11T10:43:13.600Z"
   },
   {
    "duration": 1617609,
    "start_time": "2023-07-11T10:43:25.902Z"
   },
   {
    "duration": 49,
    "start_time": "2023-07-11T11:11:34.143Z"
   },
   {
    "duration": 6270,
    "start_time": "2023-07-11T11:11:41.019Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-11T11:11:49.927Z"
   },
   {
    "duration": 2293931,
    "start_time": "2023-07-11T11:11:57.421Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-11T11:51:44.514Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
